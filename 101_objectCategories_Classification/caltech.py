# -*- coding: utf-8 -*-
"""caltech.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1m0oiqv8fK-yg--oXgmbPtXioKm7wtEjr
"""

!git clone https://github.com/pruvi007/ML_Datasets.git

"""
create independent AE's
then (784,256)->(256,128)->(128,10)
finally combine and then fine tune the weights by stacking
"""

import keras
from keras.models import Sequential
from keras.layers import Dense,Activation,Dropout,Flatten,Conv2D,MaxPooling2D
from keras.layers.normalization import BatchNormalization
import numpy as np 
from matplotlib import pyplot as plt 
from matplotlib import style
import os
from tqdm import tqdm 
import cv2
import random
from random import shuffle
from keras.applications.vgg16 import VGG16
from keras.applications.vgg16 import preprocess_input
from keras.applications.vgg16 import decode_predictions
from keras.utils import to_categorical
from keras.models import Model,Sequential

style.use('fivethirtyeight')

IMG_SIZE = 224
num_classes = 102
labels = []
print("Gathering Data......")
TRAIN_DIR = "ML_Datasets/101_ObjectCategories/"
for subdir,dirs,files in os.walk(TRAIN_DIR):
	for file in files:
		labels.append(subdir)
	labels = list(set(labels))
# print(len(labels))

# getting class_name and class_labels (0-101)
cl = []
for i in range(len(labels)):
	st = labels[i].split('/')
	cl.append([st[len(st)-1],i])
# print(cl)

train = []
test = []
level = 0
for name,num in cl: 
	new_dir = TRAIN_DIR+name
	c=0
	for subdir,dirs,imgs in os.walk(new_dir):
		for img in imgs:
			c+=1
# 	print(c)
	percent_test = int(c/4)
	percent_train = c - percent_test
	flag = np.zeros(c+1																																																																																	)

	for i in range(percent_test):
		x = random.randint(0,c)
		flag[x]=1
	c = 0
  
	for subdir,dirs,imgs in os.walk(new_dir):
		for img in imgs:
			# print(imreadg)
			img_name = name+" "+str(c)
			# print(img_name)																																																																																																																																																																																																																	
			path = os.path.join(new_dir,img)
			img = cv2.imread(path)
			img = cv2.resize(img,(IMG_SIZE,IMG_SIZE))
			if flag[c]==0:
				train.append([np.array(img),np.array(img_name),level])
			else:
				test.append([np.array(img),np.array(img_name),level])
			c+=1
	level+=1
  

print(len(train))
print(len(test))
shuffle(train)
print("Gathering Done.")

# for i in range(10):
#   print(train[i][1],train[i][2])

# creating one-hot vectors (102 length)
y_train = []
for i in range(len(train)):
#     temp = np.zeros(102)
#     temp[train[i][2]]=1
    y_train.append(train[i][2])
y_train = to_categorical(y_train)

x_train=[]
for i in range(len(train)):
    x_train.append(train[i][0])
x_train = np.array(x_train)
x_train = x_train.reshape(-1,224,224,3)
# x_train /= 255

x_test = []
for i in range(len(test)):
    x_test.append(test[i][0])
x_test = np.array(x_test)
x_test = x_test.reshape(-1,224,224,3)
# x_test /= 255

y_test = []
for i in range(len(test)):
#     temp = np.zeros(102)
#     temp[test[i][2]]=1
    y_test.append(test[i][2])
y_test = to_categorical(y_test)



vgg = VGG16()
for layer in vgg.layers[:-1]:
    layer.trainable=False
model = Sequential()
model.add(vgg)

model.add(Dense(102,activation='softmax'))
model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])
model.fit(x_train,y_train,epochs=10,batch_size=128,validation_data = (x_test,y_test))
score = model.evaluate(x_test,y_test)
print("Acc: ",100*score[1])
c=0
# for img in x_train:
#     image = img.reshape((1, img.shape[0], img.shape[1], img.shape[2]))
#     # prepare the image for the VGG model
#     image = preprocess_input(image)
#     yhat = model.predict(image)
#     label = decode_predictions(yhat)
#     label = label[0][0]
#     print(train[c][1],label[1],label[2])
#     c+=1

