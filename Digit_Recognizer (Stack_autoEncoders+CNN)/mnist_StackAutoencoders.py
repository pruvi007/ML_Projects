# -*- coding: utf-8 -*-
"""stacked2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1piBMVGWd88eoNrKP7tjF6oE79QZIx-z-
"""

!git clone https://github.com/pruvi007/ML_Datasets.git

import numpy as np
from keras.datasets import mnist
from keras.models import Model,Sequential,load_model
from keras.layers.core import Dense, Dropout, Activation,Layer
from keras.layers import Input
from keras.optimizers import SGD, Adam, RMSprop, Adagrad, Adadelta
from keras.utils import to_categorical
from sklearn.metrics import accuracy_score,classification_report, confusion_matrix
from keras.callbacks import ModelCheckpoint

from matplotlib import pyplot as plt
from matplotlib import style
import tensorflow as tf 

style.use('fivethirtyeight')

batch_size = 100
nb_classes = 10
nb_epoch = 10

# optimizers
adg = Adagrad()
sgd = SGD()
rms = RMSprop()

(X_train, y_train), (X_test, y_test) = mnist.load_data()

X_train = X_train.reshape(-1,(28*28))
X_test = X_test.reshape(-1,(28*28))
X_train = X_train.astype("float64")
X_test = X_test.astype("float64")
print(X_train.shape)
X_train /= 255
X_test /= 255

Y_train = to_categorical(y_train, nb_classes)
Y_test = to_categorical(y_test, nb_classes)


A = []
# following are the independent ENCODERS
for itr in range(4):
#     itr=2
    if itr==0:
        print("Encoders:  ",itr+1)
        input_img = Input(shape=(784,))
        # encoder 1
        enc1 = Dense(256,activation='relu')(input_img)
        # output with softmax prob.
        decode1 = Dense(784,activation='sigmoid')(enc1)
        encoder1 = Model(input_img,enc1)
        auto_enc1 = Model(input_img,decode1)
        auto_enc1.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])
        auto_enc1.fit(X_train,X_train,epochs=5,batch_size=128)

        encoder1.save('Encoder1')
        input1 = encoder1.predict(X_train)

        # this is the combined model with a softmax layer to fine tune the weights.
        final = Sequential()
        x = load_model('Encoder1')

        final.add(x)
        final.add(Dense(10,activation='softmax'))

        final.compile(optimizer='adam', loss='binary_crossentropy',metrics=['accuracy'])  
        final.fit(X_train, Y_train, epochs=10,batch_size=128,validation_data = (X_test,Y_test))
        
        acc = final.evaluate(X_test,Y_test)
        A.append(100*acc[1])
    #     print("Accuracy: ",100*acc[1])
    
    elif itr==1:
        print("Encoders:  ",itr+1)
        input_img = Input(shape=(784,))
        # encoder 1
        enc1 = Dense(256,activation='relu')(input_img)
        # output with softmax prob.
        decode1 = Dense(784,activation='sigmoid')(enc1)
        encoder1 = Model(input_img,enc1)
        auto_enc1 = Model(input_img,decode1)
        auto_enc1.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])
        auto_enc1.fit(X_train,X_train,epochs=5,batch_size=128)

        encoder1.save('Encoder1')
        input1 = encoder1.predict(X_train)
        

        input_img = Input(shape=(input1.shape[1],))
        # encoder 2
        enc1 = Dense(128,activation='relu')(input_img)
        # output with softmax prob.
        decode1 = Dense(input1.shape[1],activation='sigmoid')(enc1)
        encoder2 = Model(input_img,enc1)
        auto_enc2 = Model(input_img,decode1)
        auto_enc2.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])
        auto_enc2.fit(input1,input1,epochs=5,batch_size=128)

        encoder2.save('Encoder2')
        input2 = encoder2.predict(input1)
        
        # this is the combined model with a softmax layer to fine tune the weights.
        final = Sequential()
        x = load_model('Encoder1')

        final.add(x)
        x = load_model('Encoder2')
        final.add(x)
        final.add(Dense(10,activation='softmax'))

        final.compile(optimizer='adam', loss='binary_crossentropy',metrics=['accuracy'])  
        final.fit(X_train, Y_train, epochs=10,batch_size=128,validation_data = (X_test,Y_test))
        
        acc = final.evaluate(X_test,Y_test)
        A.append(100*acc[1])
    #     print("Accuracy: ",100*acc[1])
    
    elif itr==2:
        print("Encoders:  ",itr+1)
        input_img = Input(shape=(784,))
        # encoder 1
        enc1 = Dense(256,activation='relu')(input_img)
        # output with softmax prob.
        decode1 = Dense(784,activation='sigmoid')(enc1)
        encoder1 = Model(input_img,enc1)
        auto_enc1 = Model(input_img,decode1)
        auto_enc1.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])
        auto_enc1.fit(X_train,X_train,epochs=5,batch_size=128)

        encoder1.save('Encoder1')
        input1 = encoder1.predict(X_train)
        

        input_img = Input(shape=(input1.shape[1],))
        # encoder 2
        enc1 = Dense(128,activation='relu')(input_img)
        # output with softmax prob.
        decode1 = Dense(input1.shape[1],activation='sigmoid')(enc1)
        encoder2 = Model(input_img,enc1)
        auto_enc2 = Model(input_img,decode1)
        auto_enc2.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])
        auto_enc2.fit(input1,input1,epochs=5,batch_size=128)

        encoder2.save('Encoder2')
        input2 = encoder2.predict(input1)
        
        

        input_img = Input(shape=(input2.shape[1],))
        # encoder 3
        enc1 = Dense(64,activation='relu')(input_img)
        decode1 = Dense(input2.shape[1],activation='sigmoid')(enc1)
        encoder3 = Model(input_img,enc1)
        auto_enc3 = Model(input_img,decode1)
        auto_enc3.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])
        auto_enc3.fit(input2,input2,epochs=5,batch_size=128)

        encoder3.save('Encoder3')
        
        
         # this is the combined model with a softmax layer to fine tune the weights.
        final = Sequential()
        x = load_model('Encoder1')

        final.add(x)
        x = load_model('Encoder2')
        final.add(x)
        x = load_model('Encoder3')
        final.add(x)
        
        
        final.add(Dense(10,activation='softmax'))

        final.compile(optimizer='adam', loss='binary_crossentropy',metrics=['accuracy'])  
        final.fit(X_train, Y_train, epochs=10,batch_size=128,validation_data = (X_test,Y_test))
        predictions = final.predict(X_test)
        actual = []
        for pred in Y_test:
            actual.append(np.argmax(pred)-1)
        my = []
        for pred in predictions:
            my.append(np.argmax(pred)-1)
        acc = final.evaluate(X_test,Y_test)
        A.append(100*acc[1])
    #     print("Accuracy: ",100*acc[1])
        print("Confusion_Matrix: Encoders = 3: ")
        print(confusion_matrix(actual,my))
    else:
        print("Encoders:  ",itr+1)
        input_img = Input(shape=(784,))
        # encoder 1
        enc1 = Dense(256,activation='relu')(input_img)
        # output with softmax prob.
        decode1 = Dense(784,activation='sigmoid')(enc1)
        encoder1 = Model(input_img,enc1)
        auto_enc1 = Model(input_img,decode1)
        auto_enc1.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])
        auto_enc1.fit(X_train,X_train,epochs=5,batch_size=128)

        encoder1.save('Encoder1')
        input1 = encoder1.predict(X_train)
        

        input_img = Input(shape=(input1.shape[1],))
        # encoder 2
        enc1 = Dense(128,activation='relu')(input_img)
        # output with softmax prob.
        decode1 = Dense(input1.shape[1],activation='sigmoid')(enc1)
        encoder2 = Model(input_img,enc1)
        auto_enc2 = Model(input_img,decode1)
        auto_enc2.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])
        auto_enc2.fit(input1,input1,epochs=5,batch_size=128)

        encoder2.save('Encoder2')
        input2 = encoder2.predict(input1)
        
        

        input_img = Input(shape=(input2.shape[1],))
        # encoder 3
        enc1 = Dense(64,activation='relu')(input_img)
        decode1 = Dense(input2.shape[1],activation='sigmoid')(enc1)
        encoder3 = Model(input_img,enc1)
        auto_enc3 = Model(input_img,decode1)
        auto_enc3.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])
        auto_enc3.fit(input2,input2,epochs=5,batch_size=128)

        encoder3.save('Encoder3')
        input3 = encoder3.predict(input2)
        
        input_img = Input(shape=(input3.shape[1],))
        # encoder 3
        enc1 = Dense(32,activation='relu')(input_img)
        decode1 = Dense(input3.shape[1],activation='sigmoid')(enc1)
        encoder4 = Model(input_img,enc1)
        auto_enc4 = Model(input_img,decode1)
        auto_enc4.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])
        auto_enc4.fit(input3,input3,epochs=5,batch_size=128)

        encoder4.save('Encoder4')
        
         # this is the combined model with a softmax layer to fine tune the weights.
        final = Sequential()
        x = load_model('Encoder1')

        final.add(x)
        x = load_model('Encoder2')
        final.add(x)
        x = load_model('Encoder3')
        final.add(x)
        x = load_model('Encoder4')
        final.add(x)
        final.add(Dense(10,activation='softmax'))

        final.compile(optimizer='adam', loss='binary_crossentropy',metrics=['accuracy'])  
        final.fit(X_train, Y_train, epochs=10,batch_size=128,validation_data = (X_test,Y_test))
        
        acc = final.evaluate(X_test,Y_test)
        A.append(100*acc[1])


plt.plot(np.arange(4)+1,A)

plt.title("Stacked AutoEncoders")
plt.xlabel("No. of Encoders")
plt.ylabel("Accuracy")
plt.show()

avg = np.sum(A)/4
print("Average Accuracy: ",avg)