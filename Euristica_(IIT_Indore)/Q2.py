# -*- coding: utf-8 -*-
"""Q2_euristica.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1s4-qRobHJzuoLlj-Vrvs4XkIdP9LmR-I
"""

import numpy as np 
import pandas as pd 
from sklearn import preprocessing
from sklearn.model_selection import train_test_split
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.neighbors import KNeighborsClassifier  
from sklearn.svm import SVC 
from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import AdaBoostClassifier
from sklearn.decomposition import PCA


data = pd.read_csv('data_train.csv')
for key,value in data.iteritems():
	mean = data[key].mean()
	# print(mean)
	data[key] = data[key].fillna(mean)
	for i in range(len(data[key])):
		if data[key][i]==0:
			data[key][i]=mean
	# print(key)
# print(data.head(50))
data = data.values
print(len(data))
features = data[:,0:10]
labels = data[:,-1]
features = preprocessing.scale(features)


# print(labels)
# print(features[1])
# X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.10)


svclassifier = SVC(kernel='rbf') 
svclassifier.fit(features,labels) 



mlp = MLPClassifier(hidden_layer_sizes=(128,64,32),max_iter=1000,activation='relu')
mlp.fit(features,labels)

AdaBoost = AdaBoostClassifier(n_estimators=400,learning_rate=0.1,algorithm='SAMME')
AdaBoost.fit(features,labels)

# prediction1 = knn.predict(X_test) 
# prediction2 = svclassifier.predict(X_test)
# prediction3 = forest.predict(X_test)



# c = 0
# for i in range(len(prediction1)):
# 	p=[]
# 	p.append(int(round(prediction1[i])))
# 	p.append(int(round(prediction2[i])))
# 	p.append(int(round(prediction3[i])))
	
# # 	print(p)
	
# 	flag = np.zeros(7)
# 	for val in p:
# 		flag[val-1]+=1
# 	ind = np.argmax(flag) +1
# 	if ind == y_test[i]:
# 		c+=1

# print("Accuracy:  ",100*c/len(prediction1))

data = pd.read_csv('data_test.csv')
for key,value in data.iteritems():
	mean = data[key].mean()
	# print(mean)
	data[key] = data[key].fillna(mean)
	for i in range(len(data[key])):
		if data[key][i]==0:
			data[key][i]=mean
	# print(key)
# print(data.head(50))
data = data.values
print(len(data))
features = preprocessing.scale(data)


# prediction1 = knn.predict(features) 
prediction2 = svclassifier.predict(features)
# prediction3 = forest.predict(features)
prediction4 = mlp.predict(features)
prediction5 = AdaBoost.predict(features)
# print(prediction2)
with open("submission.csv", "w") as f:	
    for i in range(len(prediction2)):
        p=[]
#         p.append(int(round(prediction1[i])))
        p.append(int(round(prediction2[i])))
#         p.append(int(round(prediction3[i])))
        p.append(int(round(prediction4[i])))
        p.append(int(round(prediction5[i])))

    # 	print(p)

        flag = np.zeros(7)
        for val in p:
            flag[val-1]+=1
        ind = np.argmax(flag) +1
        f.write(str(ind))
        f.write('\n')
