# -*- coding: utf-8 -*-
"""Frame-LSTM.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11TBn-GakK1BFkMYROsNL1xlrejwHfA5e
"""
"""
Similar approach as the mentioned in ImageSeng+LSTM.py 
Instead of Autoencoders used PCA to reduce the number of features.
Feed to LSTM.

"""
!git clone https://github.com/pruvi007/ML_Datasets.git

import cv2
import os
import time
import sys
import argparse
import numpy as np 
import random

MAIN = "ML_Datasets/Videos/"
OP = "/media/diablo/Study Vids/VIDEOS_ENGINE/ML/python_ml_me/LSTM_project/Data"

# **********************************************************************************************
start = time.time()
print("Creating Dataset")

train_data = []
test_data = []
actions = ["boxing","handclapping","handwaving","jogging","walking"]

count = 0
level=0
for act in actions:
    folder = MAIN + act
    c = 0
    for subdir,dirs,files in os.walk(folder):       
        for file in files:
            c+=1
        
    test_percent = int(c/2)
    train_percent = c-test_percent

    flag = np.zeros(c+1)
    for x in range(test_percent):
        r = random.randint(0,c)
        flag[r]=1
    c=0
    
    for subdir,dirs,files in os.walk(folder):       
        for file in files:
            vid = folder+"/"+file
            s = vid.split("/")
            action_name = s[len(s)-2]
            if flag[c]==1:
                test_data.append([vid,action_name,level])
            else:
                train_data.append([vid,action_name,level])
            c+=1
    level+=1
print(len(train_data))
print(len(test_data))

# for i in range(len(train_data)):
#     print(train_data[i])
# print()
# for i in range(len(test_data)):
#     print(test_data[i])

np.random.shuffle(train_data)
np.random.shuffle(test_data)

train = []
test = []

for i in range(len(train_data)):
    train.append([train_data[i][0],train_data[i][2]])
for i in range(len(test_data)):
    test.append([test_data[i][0],test_data[i][2]])

# for i in range(len(train)):
#     print(train[i])
# print()
# for i in range(len(test)):
#     print(test[i])           

print(train[1])
end = time.time()

print("Datsets Created. Done! Time: ",end - start)
# *************************************************************************************************

import cv2
from keras.utils import to_categorical
from keras.models import Model,Sequential,load_model
from keras.layers import Dense,Conv3D,Dropout,Flatten,MaxPooling3D,LSTM,Input,Conv2D,MaxPooling2D
import os




def extract_human(path):
    BLUR = 21
    CANNY_THRESH_1 = 10
    CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,1.0) # In BGR format

    #== Processing =======================================================================

    #-- Read image -----------------------------------------------------------------------
    img = cv2.imread(path)
    if type(img).__name__ != "NoneType":
        
        gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)

        #-- Edge detection -------------------------------------------------------------------
        edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)
        edges = cv2.dilate(edges, None)
        edges = cv2.erode(edges, None)

        #-- Find contours in edges, sort by area ---------------------------------------------
        contour_info = []
        _,contours,bbb = cv2.findContours(edges, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)
        for c in contours:
            contour_info.append((
                c,
                cv2.isContourConvex(c),
                cv2.contourArea(c),
            ))
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)
        if(len(contours)>0):
            max_contour = contour_info[0]
        else:
            return -1

        #-- Create empty mask, draw filled polygon on it corresponding to largest contour ----
        # Mask is black, polygon is white
        mask = np.zeros(edges.shape)
        cv2.fillConvexPoly(mask, max_contour[0], (255))

        #-- Smooth mask, then blur it --------------------------------------------------------
        mask = cv2.dilate(mask, None, iterations=MASK_DILATE_ITER)
        mask = cv2.erode(mask, None, iterations=MASK_ERODE_ITER)
        mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)
        mask_stack = np.dstack([mask]*3)    # Create 3-channel alpha mask

        #-- Blend masked img into MASK_COLOR background --------------------------------------
        mask_stack  = mask_stack.astype('float32') / 255.0          # Use float matrices, 
        img         = img.astype('float32') / 255.0                 #  for easy blending

        masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR) # Blend
        masked = (masked * 255).astype('uint8')                     # Convert back to 8-bit 

    #     cv2.imshow('img', masked)                                   # Display
        # cv2.waitKey()

        #cv2.imwrite('C:/Temp/person-masked.jpg', masked)           # Save

        # split image into channels
        c_red, c_green, c_blue = cv2.split(img)

        # merge with mask got on one of a previous steps
        img_a = cv2.merge((c_red, c_green, c_blue, mask.astype('float32') / 255.0))

        #show on screen (optional in jupiter)
        #%matplotlib inline
    #     plt.imshow(img_a)
    #     plt.show()

        # save to disk
    #     cv2.imwrite('girl_1.png', img_a*255)

        # or the same using plt
    #     plt.imsave('girl_2.png', img_a)
        return img_a
    return -1


# ********************************************************************
# train the data 

import shutil

img_x = []
img_y = []
num_of_frames = 100
dim_x = 100
dim_y = 100

# shutil.rmtree("images")
if not os.path.exists("images"):
    os.makedirs("images")

pic=0
for i in  range(len(train)):
    
    vidObj = cv2.VideoCapture(train[i][0])
    label = train[i][1]
    s = train[i][0].split("/")
#     print(label)
    Y_label= to_categorical(label, 6)
#     print(Y_label)
    img_y.append(Y_label)
    success = 1
    count=0
    
    while success: 
#         vidObj.set(cv2.CAP_PROP_POS_MSEC,(count*200))	#0.2sec frames 
        success, image = vidObj.read()
#         if type(image).__name__!="NoneType":
        image = cv2.cvtColor(image,cv2.COLOR_BGR2GRAY)
            # Saves the frames with frame-count 

        cv2.imwrite("images/"+s[2]+"%d.jpg" % pic, image) 
        count += 1
        pic+=1
        if count==num_of_frames:
            break

for subdir,dirs,files in os.walk("images"): 
    for file in files:
#         print(file)
        ext_img = extract_human(file)
#         if type(ext_img).__name__!='int':
        ext_img = ext_img/255
        img_x.append(cv2.resize(ext_img,(dim_x,dim_y),interpolation = cv2.INTER_LINEAR))
        
#    
    
    
#     print(len(imgs))
    
#     model.fit(imgs, Y_label, epochs=5,batch_size=64)
print(len(img_y))
print(len(img_x))
img_x = np.array(img_x)
img_y = np.array(img_y)
print(img_x.shape)
print(img_y.shape)
num = int(img_x.shape[0]/num_of_frames)
img_x = np.reshape(img_x,(num,num_of_frames,dim_x,dim_y))
print(img_x.shape)

# shutil.rmtree("images")

img_x1= img_x[:img_x.shape[0],:]
img_x11=np.reshape(img_x1,(img_x.shape[0],num_of_frames,dim_x*dim_y))
img_x11.shape

from sklearn.decomposition import PCA
n_comp = 64
pca = PCA(n_components=n_comp)
finalimg_x=[]
#principalComponents = pca.fit_transform(img_x11)
for i in range(img_x11.shape[0]):
  principalComponents = pca.fit_transform(np.reshape(img_x11[i,:,:],(num_of_frames,dim_x*dim_y)))
  finalimg_x.append(principalComponents)

finalimg_x=np.array(finalimg_x)  
print(finalimg_x.shape)  
print(img_y.shape)

from keras.layers.normalization import BatchNormalization



#img_x = np.load('X.npy')
# img_y = np.load('Y.npy')
print(img_y.shape)
img_y=img_y[:(img_x.shape[0]*num_of_frames),:]
finalimg_x = np.reshape(finalimg_x,(img_x1.shape[0],num_of_frames,n_comp))

# img_y = np.reshape(img_y,(num_of_frames,finalimg_x.shape[1],6))
print(finalimg_x.shape,img_y.shape)

# *******************************************************
# creating Test Dataset for validation
test_img_x = []
test_img_y = []

print("Creating test frames....")
for i in  range(len(test)):
    vidObj = cv2.VideoCapture(test[i][0])
    label = test[i][1]
#     print(label)
    Y_label= to_categorical(label, 6)
    test_img_y.append(Y_label)
    success = 1
    count=0
    while success: 
        
    #     print(Y_label)
        
#         vidObj.set(cv2.CAP_PROP_POS_MSEC,(count*200))	#0.2sec frames 
        success, image = vidObj.read()
        image = cv2.cvtColor(image,cv2.COLOR_BGR2GRAY)
        image = image/255
        # Saves the frames with frame-count 
        test_img_x.append(cv2.resize(image,(dim_x,dim_y),interpolation = cv2.INTER_LINEAR))
        count += 1
        if count==num_of_frames:
            break
    
    
#     print(len(imgs))
    
#     model.fit(imgs, Y_label, epochs=5,batch_size=64)
print(len(test_img_y))

test_img_x = np.array(test_img_x)
test_img_y = np.array(test_img_y)
num = int(test_img_x.shape[0]/num_of_frames)
test_img_x = np.reshape(test_img_x,(num,num_of_frames,dim_x,dim_y))

img_x1= test_img_x[:test_img_x.shape[0],:]
img_x11=np.reshape(img_x1,(test_img_x.shape[0],num_of_frames,dim_x*dim_y))

test_finalimg_x=[]
#principalComponents = pca.fit_transform(img_x11)
for i in range(img_x11.shape[0]):
  principalComponents = pca.fit_transform(np.reshape(img_x11[i,:,:],(num_of_frames,dim_x*dim_y)))
  test_finalimg_x.append(principalComponents)
    
test_finalimg_x=np.array(test_finalimg_x)     
test_img_y=test_img_y[:(test_finalimg_x.shape[0]*num_of_frames),:] 
num = int(test_img_x.shape[0]/num_of_frames)
test_finalimg_x = np.reshape(test_finalimg_x,(test_finalimg_x.shape[0],num_of_frames,n_comp))

test_img_y = np.array(test_img_y)
# *********************************************************************************







model = Sequential()

model.add(LSTM(25,input_shape = (num_of_frames,n_comp),return_sequences = True,dropout = 0.4))
model.add(Flatten())
model.add(Dense(6,activation='softmax'))
#model.add(BatchNormalization())
model.compile(optimizer='adam', loss='binary_crossentropy',metrics=['accuracy'])  
model.summary()

model.fit(finalimg_x, img_y, epochs=40,batch_size=64,validation_data=(test_finalimg_x, test_img_y))

