# -*- coding: utf-8 -*-
"""Frame+encoder+LSTM.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1YLxegvaiaMEqo4YuJ5-8p76YRoy-sUsk
"""

"""
Segment the image to focus only on the human to get better features. Neglecting the background. (Used Canny Edge Detector)
After segmentation use autoencoders to get the reduced features.
feed to LSTM

"""
!git clone https://github.com/pruvi007/ML_Datasets.git

import cv2
import os
import time
import sys
import argparse
import numpy as np 
import random

MAIN = "ML_Datasets/Videos/"
OP = "/media/diablo/Study Vids/VIDEOS_ENGINE/ML/python_ml_me/LSTM_project/Data"

# **********************************************************************************************
start = time.time()
print("Creating Dataset")

train_data = []
test_data = []
actions = ["boxing","handclapping","handwaving","jogging","walking"]

count = 0
level=0
for act in actions:
    folder = MAIN + act
    c = 0
    for subdir,dirs,files in os.walk(folder):       
        for file in files:
            c+=1
        
    test_percent = int(c/2)
    train_percent = c-test_percent

    flag = np.zeros(c+1)
    for x in range(test_percent):
        r = random.randint(0,c)
        flag[r]=1
    c=0
    
    for subdir,dirs,files in os.walk(folder):       
        for file in files:
            vid = folder+"/"+file
            s = vid.split("/")
            action_name = s[len(s)-2]
            if flag[c]==1:
                test_data.append([vid,action_name,level])
            else:
                train_data.append([vid,action_name,level])
            c+=1
    level+=1
print(len(train_data))
print(len(test_data))

# for i in range(len(train_data)):
#     print(train_data[i])
# print()
# for i in range(len(test_data)):
#     print(test_data[i])

np.random.shuffle(train_data)
np.random.shuffle(test_data)

train = []
test = []

for i in range(len(train_data)):
    train.append([train_data[i][0],train_data[i][2]])
for i in range(len(test_data)):
    test.append([test_data[i][0],test_data[i][2]])

# for i in range(len(train)):
#     print(train[i])
# print()
# for i in range(len(test)):
#     print(test[i])           

print(train[1])
end = time.time()

print("Datsets Created. Done! Time: ",end - start)
# *************************************************************************************************

import cv2
from keras.utils import to_categorical
from keras.models import Model,Sequential,load_model
from keras.layers import Dense,Conv3D,Dropout,Flatten,MaxPooling3D,LSTM,Input,Conv2D,MaxPooling2D
import os




def extract_human(path):
    BLUR = 21
    CANNY_THRESH_1 = 10
    CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,1.0) # In BGR format

    #== Processing =======================================================================

    #-- Read image -----------------------------------------------------------------------
    img = cv2.imread(path)
    if type(img).__name__ != "NoneType":
        
        gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)

        #-- Edge detection -------------------------------------------------------------------
        edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)
        edges = cv2.dilate(edges, None)
        edges = cv2.erode(edges, None)

        #-- Find contours in edges, sort by area ---------------------------------------------
        contour_info = []
        _,contours,bbb = cv2.findContours(edges, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)
        for c in contours:
            contour_info.append((
                c,
                cv2.isContourConvex(c),
                cv2.contourArea(c),
            ))
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)
        if(len(contours)>0):
            max_contour = contour_info[0]
        else:
            return -1

        #-- Create empty mask, draw filled polygon on it corresponding to largest contour ----
        # Mask is black, polygon is white
        mask = np.zeros(edges.shape)
        cv2.fillConvexPoly(mask, max_contour[0], (255))

        #-- Smooth mask, then blur it --------------------------------------------------------
        mask = cv2.dilate(mask, None, iterations=MASK_DILATE_ITER)
        mask = cv2.erode(mask, None, iterations=MASK_ERODE_ITER)
        mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)
        mask_stack = np.dstack([mask]*3)    # Create 3-channel alpha mask

        #-- Blend masked img into MASK_COLOR background --------------------------------------
        mask_stack  = mask_stack.astype('float32') / 255.0          # Use float matrices, 
        img         = img.astype('float32') / 255.0                 #  for easy blending

        masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR) # Blend
        masked = (masked * 255).astype('uint8')                     # Convert back to 8-bit 

    #     cv2.imshow('img', masked)                                   # Display
        # cv2.waitKey()

        #cv2.imwrite('C:/Temp/person-masked.jpg', masked)           # Save

        # split image into channels
        c_red, c_green, c_blue = cv2.split(img)

        # merge with mask got on one of a previous steps
        img_a = cv2.merge((c_red, c_green, c_blue, mask.astype('float32') / 255.0))

        #show on screen (optional in jupiter)
        #%matplotlib inline
    #     plt.imshow(img_a)
    #     plt.show()

        # save to disk
    #     cv2.imwrite('girl_1.png', img_a*255)

        # or the same using plt
    #     plt.imsave('girl_2.png', img_a)
        return img_a
    return -1


# ********************************************************************
# train the data 

import shutil

img_x = []
img_y = []
num_of_frames = 100
dim_x = 100
dim_y = 100

if os.path.exists("images"):
    shutil.rmtree("images")
# shutil.rmtree("images")
if not os.path.exists("images"):
    os.makedirs("images")

pic=0
for i in  range(len(train)):
    
    vidObj = cv2.VideoCapture(train[i][0])
    label = train[i][1]
    s = train[i][0].split("/")
#     print(label)
    Y_label= to_categorical(label, 6)
#     print(Y_label)
    img_y.append(Y_label)
    success = 1
    count=0
    
    while success: 
#         vidObj.set(cv2.CAP_PROP_POS_MSEC,(count*200))	#0.2sec frames 
        success, image = vidObj.read()
#         if type(image).__name__!="NoneType":
        image = cv2.cvtColor(image,cv2.COLOR_BGR2GRAY)
            # Saves the frames with frame-count 
#         image = image/255
        cv2.imwrite("images/"+s[2]+"%d.jpg" % pic, image) 
        count += 1
        pic+=1
        if count==num_of_frames:
            break

for subdir,dirs,files in os.walk("images"): 
    for file in files:
#         print(file)
        ext_img = extract_human(file)
#         if type(ext_img).__name__!='int':
#         ext_img = ext_img/255
        img_x.append(cv2.resize(ext_img,(dim_x,dim_y),interpolation = cv2.INTER_LINEAR))
        
#    
    
    
#     print(len(imgs))
    
#     model.fit(imgs, Y_label, epochs=5,batch_size=64)
print(len(img_y))
print(len(img_x))
img_x = np.array(img_x)
img_y = np.array(img_y)
print(img_x.shape)
print(img_y.shape)
num = int(img_x.shape[0]/num_of_frames)
# img_x = np.reshape(img_x,(num,num_of_frames,dim_x,dim_y))
img_x = np.reshape(img_x,(img_x.shape[0],img_x.shape[1]*img_x.shape[2]))
print(img_x.shape)

# test_img_x = np.reshape(test_img_x,(test_img_x.shape[0],test_img_x.shape[1]*test_img_x.shape[2]))


# shutil.rmtree("images")

# *******************************************
# Stacked AutEncoder to generate TRAIN
input_img = Input(shape=(img_x.shape[1],))
# encoder 1
enc1 = Dense(512,activation='relu')(input_img)
# output with softmax prob.
decode1 = Dense(img_x.shape[1],activation='sigmoid')(enc1)
encoder1 = Model(input_img,enc1)
encoder1.summary()
auto_enc1 = Model(input_img,decode1)
auto_enc1.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])
auto_enc1.fit(img_x,img_x,epochs=5,batch_size=128)

encoder1.save('Encoder1')
input1 = encoder1.predict(img_x)

input_img = Input(shape=(input1.shape[1],))
# encoder 2
enc1 = Dense(128,activation='relu')(input_img)
# output with softmax prob.
decode1 = Dense(input1.shape[1],activation='sigmoid')(enc1)
encoder2 = Model(input_img,enc1)
auto_enc2 = Model(input_img,decode1)
auto_enc2.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])
auto_enc2.fit(input1,input1,epochs=5,batch_size=128)

encoder2.save('Encoder2')
input2 = encoder2.predict(input1)

input2 = np.reshape(input2,(input2.shape[0]//num_of_frames,num_of_frames,input2.shape[1]))

print(input2.shape)
# this is the combined model with a softmax layer to fine tune the weights.
# **************************************************************



# *******************
# *************TEST CREATION***********
if os.path.exists("images2"):
    shutil.rmtree("images2")
test_img_x = []
test_img_y = []

if os.path.exists("images2"):
    shutil.rmtree("images2")
    
if not os.path.exists("images2"):
    os.makedirs("images2")

pic=0
print("Creating test frames....")
for i in  range(len(test)):
    vidObj = cv2.VideoCapture(test[i][0])
    label = test[i][1]
    s = test[i][0].split("/")
#     print(label)
    Y_label= to_categorical(label, 6)
    test_img_y.append(Y_label)
    success = 1
    count=0
    while success: 
        
    #     print(Y_label)
        
#         vidObj.set(cv2.CAP_PROP_POS_MSEC,(count*200))	#0.2sec frames 
        success, image = vidObj.read()
        image = cv2.cvtColor(image,cv2.COLOR_BGR2GRAY)
#         image = image/255
        cv2.imwrite("images2/"+s[2]+"%d.jpg" % pic, image)
        # Saves the frames with frame-count 
#         test_img_x.append(cv2.resize(image,(dim_x,dim_y),interpolation = cv2.INTER_LINEAR))
        count += 1
        pic+=1
        if count==num_of_frames:
            break
        
        
for subdir,dirs,files in os.walk("images2"): 
    for file in files:
#         print(file)
        ext_img = extract_human(file)
#         if type(ext_img).__name__!='int':
#         ext_img = ext_img/255
        test_img_x.append(cv2.resize(ext_img,(dim_x,dim_y),interpolation = cv2.INTER_LINEAR))

test_img_x = np.array(test_img_x)
test_img_y = np.array(test_img_y)
print(test_img_x.shape)
print(test_img_y.shape)
num = int(test_img_x.shape[0]/num_of_frames)
test_img_x = np.reshape(test_img_x,(test_img_x.shape[0],test_img_x.shape[1]*test_img_x.shape[2]))


test_input1 = encoder1.predict(test_img_x)

test_input2 = encoder2.predict(test_input1)
print(test_input2.shape)
test_input2 = np.reshape(test_input2,(test_input2.shape[0]//num_of_frames,num_of_frames,test_input2.shape[1]))
# ***********************************************************************************
# *****************************************************

final = Sequential()
# x = load_model('Encoder1')
final.add(LSTM(50,input_shape=(num_of_frames,input2.shape[2]),return_sequences=True,dropout=0.2))
final.add(Flatten())
final.add(Dense(6,activation='softmax'))

final.compile(optimizer='adam', loss='binary_crossentropy',metrics=['accuracy'])  
history = final.fit(input2, img_y, epochs=100,batch_size=128,validation_data=(test_input2,test_img_y))

import matplotlib.pyplot as plt
from matplotlib import style

plt.rcParams['figure.figsize'] = [10, 10]
style.use('fivethirtyeight')
# print(history.history)
loss = history.history['loss']
acc = history.history['acc']
val_acc = history.history['val_acc']
print(loss)
plt.plot(np.arange(100)+1,loss,linewidth='2')

plt.title('Prop. MODEL-2 (AE+FRAME+LSTM)')
plt.xlabel('EPOCH')
plt.ylabel('LOSS')
plt.show()
print("\n\n")
print(acc)
plt.plot(np.arange(100)+1,acc,linewidth='2',label='TRAIN')
plt.plot(np.arange(100)+1,val_acc,linewidth='2',label='TEST')
plt.title('Prop. MODEL-2 (AE+FRAME+LSTM)')
plt.xlabel('EPOCH')
plt.ylabel('ACCURACY')
plt.legend(loc='upper left')
plt.show()
